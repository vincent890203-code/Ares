# æª”æ¡ˆä½ç½®ï¼š Ares/brain/base.py

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime
from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score

# è¨­å®šç¹ªåœ–é¢¨æ ¼ (é€™æ˜¯ä½ åŽŸæœ¬çš„è¨­å®š)
plt.style.use('seaborn-v0_8')
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Microsoft JhengHei', 'SimHei'] 
plt.rcParams['axes.unicode_minus'] = False

@dataclass
class ClassificationResult:
    """åˆ†é¡žä»»å‹™çš„å ±å‘Š"""
    predictions: np.ndarray
    probabilities: np.ndarray
    prediction_labels: list
    timestamp: str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

@dataclass
class RegressionResult:
    """å›žæ­¸ä»»å‹™çš„å ±å‘Š"""
    predictions: np.ndarray
    actuals: np.ndarray = None 
    timestamp: str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

class BaseAlgorithm(ABC):
    def __init__(self, model_name):
        self.model = None
        self.model_name = model_name
        self.feature_names_ = None

# æª”æ¡ˆä½ç½®ï¼š Ares/brain/base.py è£¡é¢çš„ BaseAlgorithm é¡žåˆ¥ä¸‹

    def _validate_input(self, X, is_training=False):
        """å…±ç”¨çš„è¼¸å…¥æª¢æŸ¥å®ˆé–€å“¡ (å·²å‡ç´šï¼šæ”¯æ´ Numpy Array)"""
        
        # 1. è‡ªå‹•å®¹éŒ¯ï¼šå¦‚æžœæ˜¯ Numpy Arrayï¼Œç›´æŽ¥è½‰æˆ DataFrame
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X)

        # 2. ç¶“éŽè½‰æ›å¾Œï¼Œå¦‚æžœé‚„ä¸æ˜¯ DataFrame å°±çœŸçš„å ±éŒ¯
        if not isinstance(X, pd.DataFrame):
            raise TypeError(f"âŒ {self.model_name}: è«‹è¼¸å…¥ pd.DataFrame æ ¼å¼")
        
        # 3. è¨“ç·´æ¨¡å¼ï¼šè¨˜éŒ„ç‰¹å¾µåç¨± (å¦‚æžœæ˜¯ Numpy è½‰ä¾†çš„ï¼Œæ¬„ä½åæœƒæ˜¯ 0, 1, 2...)
        if is_training:
            self.feature_names_ = list(X.columns)
            return X
        
        # 4. é æ¸¬æ¨¡å¼ï¼šæª¢æŸ¥æ¨¡åž‹æ˜¯å¦å·²è¨“ç·´
        if self.feature_names_ is None:
            raise Exception("âŒ æ¨¡åž‹å°šæœªè¨“ç·´")
        
        # 5. é æ¸¬æ¨¡å¼ï¼šæ¬„ä½å°é½Šæª¢æŸ¥
        # å¦‚æžœè¼¸å…¥çš„æ˜¯ Numpy (æ¬„ä½åæ˜¯ 0, 1...)ï¼Œé€šå¸¸é•·åº¦å°äº†å°±è¡Œï¼Œé€™è£¡åšå€‹å¯¬å®¹è™•ç†
        missing = set(self.feature_names_) - set(X.columns)
        if missing:
            # åªæœ‰ç•¶ç¼ºå¤±çš„æ¬„ä½ä¸æ˜¯ "æ•¸å­—ç´¢å¼•" æ™‚æ‰å ±éŒ¯ï¼Œé¿å…å¤ªåš´æ ¼
            if not all(isinstance(col, int) for col in missing):
                 raise ValueError(f"âŒ ç¼ºå°‘æ¬„ä½: {missing}")
            
        return X[self.feature_names_]

    @abstractmethod
    def fit(self, X, y): pass

class BaseClassifier(BaseAlgorithm):
    def __init__(self, model_name, label_map):
        super().__init__(model_name)
        self.label_map = label_map

    def predict(self, X) -> ClassificationResult:
        X_val = self._validate_input(X)
        preds = self.model.predict(X_val)
        
        if hasattr(self.model, "predict_proba"):
            probs = np.max(self.model.predict_proba(X_val), axis=1)
        else:
            probs = np.zeros_like(preds, dtype=float)
            
        labels = [self.label_map.get(p, str(p)) for p in preds]
        return ClassificationResult(preds, probs, labels)

    def evaluate(self, X_test, y_test):
        res = self.predict(X_test)
        print(f"\n=== {self.model_name} åˆ†é¡žå ±å‘Š ===")
        print(classification_report(y_test, res.predictions))
        
        cm = confusion_matrix(y_test, res.predictions)
        plt.figure(figsize=(5, 4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f'{self.model_name} Confusion Matrix')
        plt.show()

class BaseRegressor(BaseAlgorithm):
    def __init__(self, model_name):
        super().__init__(model_name)

    def predict(self, X) -> RegressionResult:
        X_val = self._validate_input(X)
        preds = self.model.predict(X_val)
        return RegressionResult(predictions=preds)

    def evaluate(self, X_test, y_test):
        res = self.predict(X_test)
        mse = mean_squared_error(y_test, res.predictions)
        r2 = r2_score(y_test, res.predictions)
        
        print(f"\n=== {self.model_name} å›žæ­¸å ±å‘Š ===")
        print(f"ðŸ“‰ MSE: {mse:.4f}")
        print(f"ðŸ“Š R2 Score: {r2:.4f}")
        
        plt.figure(figsize=(6, 5))
        plt.scatter(y_test, res.predictions, alpha=0.6, color='teal')
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        plt.xlabel("True Values")
        plt.ylabel("Predictions")
        plt.title(f"{self.model_name}: Actual vs Predicted")
        plt.show()